# Machine Learning: From Scratch to Advanced ğŸš€

# ğŸŒ³ Decision Tree Algorithms (Supervised Machine Learning)

## ğŸ“Œ Classification & Regression using Decision Trees (End-to-End Workflow)

This repository contains two complete Machine Learning projects based on **Decision Tree Algorithms**, implemented using real datasets with a focus on:

âœ… Data understanding & preprocessing  
âœ… Building supervised ML models  
âœ… Training & testing workflow  
âœ… Model evaluation & interpretation  
âœ… Understanding how Decision Trees work (not treating ML as a black box)

---

## ğŸ“‚ Project Files Included

| Notebook                              | Algorithm Type           | Problem Type   | Dataset                      |
| ------------------------------------- | ------------------------ | -------------- | ---------------------------- |
| `decision-tree-algo-classifier.ipynb` | Decision Tree Classifier | Classification | Titanic Dataset (`seaborn`)  |
| `decision-tree-algo-regressor.ipynb`  | Decision Tree Regressor  | Regression     | Diabetes Dataset (`sklearn`) |

---

# ğŸ§  What is a Decision Tree?

A **Decision Tree** is a supervised learning algorithm that works by repeatedly splitting the dataset into smaller groups based on feature conditions, forming a tree-like structure.

It is widely used because it is:

âœ… Easy to understand  
âœ… Interpretable (can visualize the tree)  
âœ… Works with both numerical & categorical data  
âœ… Can solve both classification and regression problems

---

# ğŸŒ³ How Decision Trees Work (Core Idea)

A Decision Tree learns rules in the form:

> If (feature condition is true) â†’ go left  
> Else â†’ go right

Each split is chosen based on how well it separates the data.

### ğŸ”¹ For Classification:

The goal is to split data so that each leaf contains mostly one class (pure nodes).

Common splitting criteria:

- **Gini Impurity**
- **Entropy (Information Gain)**

### ğŸ”¹ For Regression:

The goal is to split data so that the target values in each leaf are as close as possible.

Common splitting criteria:

- **Mean Squared Error (MSE)**
- **Variance reduction**

---

# âœ… 1. Decision Tree Classifier Notebook

ğŸ“Œ `decision-tree-algo-classifier.ipynb`

## ğŸ” Problem Statement

Build a **classification model** to predict whether a passenger survived the Titanic disaster.

### ğŸ¯ Target Variable

- `survived`
  - `0` â†’ Did not survive
  - `1` â†’ Survived

### ğŸ“Œ Dataset Used

- Titanic dataset loaded using:

```python
sns.load_dataset("titanic")
```

---

## ğŸ› ï¸ Workflow Implemented

### 1ï¸âƒ£ Data Loading

- Loaded Titanic dataset
- Previewed dataset using `.head()`

### 2ï¸âƒ£ Data Understanding (EDA Basics)

- Checked dataset columns, structure
- Identified missing values
- Selected relevant features

### 3ï¸âƒ£ Data Preprocessing

Decision Trees require **numerical input**, so categorical features must be encoded.

Preprocessing steps included:

- Handling missing values (dropping/filling)
- Feature selection
- Encoding categorical variables into numeric form
- Train-test splitting

---

## ğŸ§ª Model Training

Model used:

```python
DecisionTreeClassifier()
```

The model learns splitting rules to classify passengers into:
âœ… Survived
âŒ Not Survived

---

## ğŸ“Š Evaluation Metrics Used

The notebook evaluates performance using:

### âœ… Accuracy Score

Accuracy tells the percentage of correct predictions:

```python
accuracy_score(y_test, y_pred)
```

### âœ… Confusion Matrix (Recommended)

A confusion matrix helps understand true/false predictions:

- True Positive
- True Negative
- False Positive
- False Negative

### âœ… Precision / Recall / F1-score (Recommended)

For better evaluation, classification models should also be checked with:

- Precision â†’ How many predicted survivors actually survived?
- Recall â†’ How many actual survivors were correctly found?
- F1-score â†’ Balance between precision & recall

---

## ğŸŒ³ Tree Visualization

The notebook visualizes the Decision Tree using:

```python
plot_tree(model, filled=True)
```

This helps interpret:

- Feature splits
- Decision rules
- Leaf node outcomes

---

## âœ… Key Learnings (Classifier)

âœ” How Decision Trees classify data using splitting rules
âœ” How feature selection affects model performance
âœ” Why encoding categorical features is important
âœ” How to evaluate classification models using accuracy
âœ” How to interpret a Decision Tree visually

---

---

# âœ… 2. Decision Tree Regressor Notebook

ğŸ“Œ `decision-tree-algo-regressor.ipynb`

## ğŸ” Problem Statement

Build a **regression model** to predict a continuous target value using the Diabetes dataset.

### ğŸ¯ Target Variable

- `target` â†’ a numeric value representing disease progression

### ğŸ“Œ Dataset Used

Diabetes dataset loaded using:

```python
load_diabetes(as_frame=True)
```

---

## ğŸ› ï¸ Workflow Implemented

### 1ï¸âƒ£ Data Loading

- Loaded Diabetes dataset into a DataFrame
- Checked dataset shape and previewed data

### 2ï¸âƒ£ Feature & Target Split

```python
X = df.drop("target", axis=1)
y = df["target"]
```

### 3ï¸âƒ£ Train-Test Split

Split data into:

- Training set â†’ used for learning patterns
- Testing set â†’ used for evaluation

---

## ğŸ§ª Model Training

Model used:

```python
DecisionTreeRegressor()
```

This model predicts numeric values by learning splitting rules that reduce error.

---

## ğŸ“Š Evaluation Metrics Used

### âœ… RÂ² Score (Coefficient of Determination)

Measures how well the model explains variance:

- `1.0` â†’ perfect prediction
- `0.0` â†’ no improvement over mean baseline
- `<0` â†’ worse than baseline

```python
r2_score(y_test, y_pred)
```

### âœ… Mean Squared Error (MSE)

Measures average squared prediction error:

```python
mean_squared_error(y_test, y_pred)
```

### âœ… RMSE (Recommended)

RMSE is the square root of MSE and is easier to interpret:

```python
mean_squared_error(y_test, y_pred, squared=False)
```

---

## ğŸŒ³ Tree Visualization

Decision tree plotted using:

```python
plot_tree(regressor, filled=True)
```

This helps understand:

- Which features influence predictions
- How splits are made
- Leaf node prediction values

---

## âœ… Key Learnings (Regressor)

âœ” How Decision Trees handle regression problems
âœ” How splitting reduces error in continuous prediction
âœ” Why overfitting is common in deep trees
âœ” How to evaluate regression models using RÂ² and MSE
âœ” Visual interpretation of regression tree structure

---

# âš ï¸ Important Concepts: Overfitting in Decision Trees

Decision Trees can easily **overfit** when they grow too deep.

### Signs of Overfitting:

- Very high training score
- Low test score

### How to Control Overfitting:

Use hyperparameters such as:

- `max_depth`
- `min_samples_split`
- `min_samples_leaf`
- `max_features`

Example:

```python
DecisionTreeClassifier(max_depth=4, min_samples_leaf=5, random_state=42)
```

---

# ğŸ“Œ Requirements (Dependencies)

Install required libraries:

```bash
pip install numpy pandas matplotlib seaborn scikit-learn
```

---

# â–¶ï¸ How to Run the Notebooks

1. Clone the repository
2. Open Jupyter Notebook / Jupyter Lab
3. Run each notebook cell-by-cell

```bash
jupyter notebook
```

---

# ğŸ“ˆ Next Improvements (Planned Enhancements)

ğŸš€ Future enhancements that can be added:

âœ… Add `classification_report` & confusion matrix for classifier
âœ… Add `RMSE` metric for regressor
âœ… Compare train vs test score to detect overfitting
âœ… Hyperparameter tuning using GridSearchCV
âœ… Feature importance visualization
âœ… Cross-validation for more reliable results
âœ… Build a reusable ML pipeline structure

---

# ğŸ§‘â€ğŸ’» Author

**Satinder Singh**
Machine Learning Learner | AI & Data Science Enthusiast
ğŸ“Œ Focused on learning by building end-to-end ML projects.

---

# â­ If you found this helpful

Feel free to â­ the repository and share feedback or suggestions for improvements!
