import pandas as pd
import numpy as np


# Series in pandas
s = pd.Series([23, 24, 25, 26])
print(s)
print(type(s))

# Indexing
print(s[0])    # 23
print(s[2])    # 25

print(s.index)     # all labels

# Custom Indexing
s2 = pd.Series([23, 24, 25, 26], index = ["Adam", "Eve", "Charlie", "Bob"])
print(s2["Eve"])    # 24
print(s2["Bob"])    # 26

# Vectorized Operations
s1 = pd.Series([1, 2, 3])
s2 = pd.Series([4, 5, 6])

print(s1 + s2)

# Mutable Values but immutable size
s = pd.Series([1, 2, 3, 4, 5])
s[0] = 100

print(s)

changed_s = s.drop(1)
print(changed_s)
print(s)


# DataFrame in pandas - using dictionary
info = {
    "Name" : ["Adam", "Eve", "Bob"],
    "Marks" : [78, 99, 85],
    "Grade" : ['B', 'O', 'A']
}

df = pd.DataFrame(info)

print(df)
print(type(df))

print(df.index)        # row labels
print(df.columns)      # column labels

# DataFrom using Numpy array
np_arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
df = pd.DataFrame(np_arr, columns=["Col1", "Col2", "Col3"])
print(df)

# DataFrom using Lists
l = [["Adam", 96], ["Eve", 75], ["Bob", 82], ["Charlie", 92]]
df = pd.DataFrame(l, columns=["Name", "Marks"])
print(df)


# pandas with csv files
data = pd.read_csv("employee_data.csv")
print(data, type(data))

# pandas with json files
json_data = pd.read_json("employee_data.json")
print(json_data, type(json_data))


# DataFrame Methods
df = pd.read_csv("employee_data.csv")

print(df.head())
print(df.tail())
print(df.sample())
print(df.info())
print(df.shape)
print(df.describe())
print(df.columns)
print(df.nunique())


df = pd.read_csv("globalAirQuality.csv")
print(type(df))


# Selecting Data

# Select columns
df["country"]
df[["city", "aqi"]]

# Select rows (by label)
df.loc[0]             # 1st row (by label)
df.loc[0:3]           # row0 to row3 - both inclusive in loc

# Select rows (by index)
df.iloc[0]            # 1st row 
df.iloc[4:7]          # 1st row 

# Select rows & columns
df.loc[0, "country"]  # 0th row & specific column
df.iloc[0, 2]         # 1th row & 3rd column (by position)

df.iloc[0, 1:5]
df.loc[0, ["country", "aqi"]]

df.loc[0:5, ["country", "city", "latitude"]]   # end is inclusive
df.iloc[0:5, 1:3]                              # end is exclusive 

# Select single scalar value
df.at[0, "city"]
df.iat[0, 2]


# Filtering Data
df[df["aqi"] > 100]
df[df["aqi"] > 100][["city", "aqi"]]

df[(df["aqi"] > 100) & (df["timestamp"] == "2025-11-04 18:25:17.554219")]
aqi_data = df[(df["aqi"] > 100) & (df["timestamp"] == "2025-11-04 18:25:17.554219")][["city", "aqi"]]

# Difference in loc & iloc - both give same value
aqi_data.loc[360] 
aqi_data.iloc[1]

# Query - returns a copy, not a view
df.query("aqi > 100 & timestamp == '2025-11-04 18:25:17.554219'")

my_country = "IN"
df.query("country == @my_country")

df["timestamp"] = pd.to_datetime(df["timestamp"])
df["date"] = df["timestamp"].dt.date
selected_date = pd.to_datetime("2025-11-04").date()

df.query("country == @my_country & date == @selected_date & aqi > 100")

# Use .copy() to avoid confusion
countries = df[["country", "aqi"]].copy()
countries.loc[0:4, "country"] = "India"

print(countries)


# Cleaning Data
df = pd.read_csv("raw_data.csv") 

# Handle Missing values
df.isnull()            # True for null values (NaN)
df.isnull().sum()      # counts missing vals per column

df.dropna()            # drops rows with missing values
df.dropna(axis = 1)    # drops cols with missing values

df.fillna(0)           # fills NaN with value
df["age"] = df["age"].fillna(df["age"].mean())           # fills column with mean

df.ffill()             # forward fill
df.bfill()             # backward fill


# Handle Duplicate values
df = pd.read_csv("raw_data.csv") 

df.duplicated()                      # True for duplicate rows
df.duplicated("country")             # True for duplicate rows in particular col
df.duplicated(["country", "gender"]) 

df.drop_duplicates()                 # drops duplicate rows


# Handle Data Types
df = pd.read_csv("raw_data.csv") 

df.dtypes                                     # dtype of each col
df2 = df.fillna(0).copy()
df2["income"] = df2["income"].astype(int)       # change dtype

# String type
df2["gender"].str.lower()                      # lower case
df2["gender"].str.upper()                      # upper case
df2["gender"].str.capitalize()                 # capitalize

df2["gender"].str.capitalize()                 # capitalize
df2["name"].str.split(" ")                     # splits into lists based on separator value 
type(df2["name"].str.split(" ")[0])            

df2["country"].str.contains("US")
df2["country"].str.contains("india", case=False)


# Transformation / Feature Engineering
df2 = df.copy()

# apply a fnx to a row or col
df2["tax"] = df2["income"].apply(lambda x : "10%" if x < 60000 else "20%")

# map values to another
new_gender_vals = {"Male": "M", "Female": "F", "Unknown": "U"}
df2["gender"] = df2["gender"].map(new_gender_vals)

# assign some new values
df2.assign(new_income = df2["income"] * 1.1)

# replace specific values
df2["country"].replace("USA", "United States of America")


# Transformation - Sorting
df2 = df.copy()

# Renaming
df2.columns = ["Id", "Name", "Age", "Country", "Gender", "Income"]
df2.rename(columns={"Income": "Salary"})
df2.rename(index={0: "first"})

# Sorting - values & index
df2.sort_values("Income")                     # sort values in ascending
df2.sort_values("Income", ascending=False)    # sort values in descending
df2.sort_values(["Age", "Income"])            # sorts for age, if age is same then sorts for income 

sorted_df2 = df2.sort_values(["Age", "Income"])
sorted_df2.sort_index()

# Reset Index
sorted_df2.reset_index()
sorted_df2.reset_index(drop=True)     # to drop original index vals

# Ranking
df2["Ranking"] = df2["Income"].rank(ascending=False, method="dense")

# Reorder
df2[["Id", "Name", "Age", "Income", "Gender", "Country", "Ranking"]]


# Task - shift id column to end
df2 = df.copy()

new_col_order = [col for col in df.columns if col != "id"] + ["id"]
df2[new_col_order]


# Writing to csv files
df = pd.read_csv("raw_data.csv") 

df.drop_duplicates(inplace=True)
df.sort_values("income", inplace=True)

df.to_csv("sorted_data.csv")   # to preserve index
df.to_csv("sorted_data.csv", index=False)   # to renew index

json_string = df.to_json()
print(json_string)


# Grouping & Aggregation
df2 = df.copy()

df2.groupby("country")["income"].mean()     # mean income for each country
df2.groupby("gender")["income"].mean()      # mean income for each gender

df2.groupby("country")["gender"].count()       # count of gender for each country
df2.groupby("gender")["income"].max()       # max income for each gender

df2.groupby("country")["income"].agg(["mean", "min", "max"])         # applies multiple aggregate functions
df2.groupby("country")["income"].aggregate(["mean", "min", "max"])   # alias

df2.groupby("country")["income"].agg(avg_salary="mean", max_salary="max")   # rename aggregate

df2.groupby("country").agg({
    "age": "mean",
    "income": "mean"
})   # aggregate on multiple cols

df2.groupby("country").agg(
    avg_age=("age", "mean"),
    avg_salary=("income", "mean")
)   # rename aggregates on multiple cols


# Melt & Pivot
df = pd.DataFrame({
    "country": ["USA", "USA", "India", "India"],
    "year": [2020, 2021, 2020, 2021],
    "sales": [100, 120, 90, 110],
    "profit": [20, 25, 18, 22]
})

melted = df.melt(
    id_vars=["country", "year"],     # columns to keep
    value_vars=["sales", "profit"],  # columns to unpivot
    var_name="metric",               # new column name for variable
    value_name="value (in Lakhs)"    # new column name for value (default is value)
)

print(melted)

original = melted.pivot(
    index=["country", "year"],   # cols that become the NEW row index - can't for just country
    columns="metric",            # col whose unique values will become the cols
    values="value (in Lakhs)"    # col whose values will fill the new df
)

print(pivoted)


# Merging & Joining
df_customers = pd.DataFrame({
    "customer_id": [1, 2, 3, 4],
    "name": ["Adam", "Bob", "Charlie", "Dave"]
})

df_orders = pd.DataFrame({
    "order_id": [101, 102, 103, 104],
    "customer_id": [2, 1, 4, 5],
    "amount": [250, 120, 300, 180]
})

pd.merge(df_customers, df_orders, on="customer_id")                  # Inner Join
pd.merge(df_customers, df_orders, on="customer_id", how="left")      # Left Join
pd.merge(df_customers, df_orders, on="customer_id", how="right")     # Right Join
pd.merge(df_customers, df_orders, on="customer_id", how="outer")     # Outer Join


# Concatenation
df1 = pd.DataFrame({
    "id": [1, 2, 3],
    "name": ["Adam", "Bob", "Charlie"]
})

df2 = pd.DataFrame({
    "id": [4, 5, 6],
    "name": ["David", "Eva", "Frank"]
})

pd.concat([df1, df2])                       # Row wise concatenation
pd.concat([df1, df2], axis=1)               # Col wise concatenation
pd.concat([df1, df2], ignore_index=True)    # Row wise concatenation - new index


# Basic Visualization

df["age"].hist()
df.plot(kind='scatter', x='age', y='income')



