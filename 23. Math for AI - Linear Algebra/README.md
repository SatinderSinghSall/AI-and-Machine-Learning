# üìä Math for AI ‚Äì Linear Algebra

# Math for AI ‚Äì Linear Algebra

Linear Algebra is the **core mathematical language of Artificial Intelligence and Machine Learning**.  
Almost everything in AI ‚Äî data, models, training, optimization ‚Äî is expressed using vectors, matrices, and linear transformations.

This document covers **Linear Algebra from basic to advanced**, with **examples and AI intuition**.

---

## Table of Contents

1. Why Linear Algebra for AI?
2. Scalars, Vectors, and Matrices
3. Vector Operations
4. Matrix Operations
5. Systems of Linear Equations
6. Vector Spaces
7. Linear Transformations
8. Matrix Properties
9. Determinants
10. Matrix Inverse
11. Rank and Null Space
12. Eigenvalues and Eigenvectors
13. Diagonalization
14. Orthogonality and Orthonormal Bases
15. Projections
16. Singular Value Decomposition (SVD)
17. Norms and Distance Measures
18. Quadratic Forms
19. Linear Algebra in Machine Learning
20. Summary Cheat Sheet

---

## 1. Why Linear Algebra for AI?

In AI:

- **Data** ‚Üí vectors and matrices
- **Models** ‚Üí matrix transformations
- **Predictions** ‚Üí dot products
- **Training** ‚Üí solving linear systems
- **Dimensionality reduction** ‚Üí eigenvalues & SVD

Example:

- An image (28√ó28 pixels) ‚Üí a **784-dimensional vector**
- A dataset ‚Üí a **matrix**
- Neural network layers ‚Üí **matrix multiplications**

---

## 2. Scalars, Vectors, and Matrices

### Scalar

A single number.

```

a = 5

```

### Vector

An ordered list of numbers.

```

v = [2, 4, 6]

```

Column vector:

```

v = ‚é°2‚é§
‚é¢4‚é•
‚é£6‚é¶

```

### Matrix

A 2D array of numbers.

```

A = ‚é°1 2‚é§
‚é£3 4‚é¶

```

**AI intuition:**

- Vector ‚Üí one data point
- Matrix ‚Üí dataset

---

## 3. Vector Operations

### Vector Addition

```

[1, 2] + [3, 4] = [4, 6]

```

### Scalar Multiplication

```

2 √ó [1, 3] = [2, 6]

```

### Dot Product

```

[1, 2] ¬∑ [3, 4] = 1√ó3 + 2√ó4 = 11

```

**Interpretation:**

- Measures similarity between vectors
- Used in cosine similarity and neural networks

---

## 4. Matrix Operations

### Matrix Addition

```

A + B = element-wise addition

```

### Matrix Multiplication

```

A (m√ón) √ó B (n√óp) = C (m√óp)

```

Example:

```

‚é°1 2‚é§ ‚é°5 6‚é§ ‚é°19 22‚é§
‚é£3 4‚é¶ √ó ‚é£7 8‚é¶ = ‚é£43 50‚é¶

```

**AI intuition:**

- Core operation in neural networks
- Each layer = matrix multiplication + bias

---

## 5. Systems of Linear Equations

Example:

```

2x + y = 5
x - y = 1

```

Matrix form:

```

Ax = b

```

```

‚é°2 1‚é§ ‚é°x‚é§ = ‚é°5‚é§
‚é£1 -1‚é¶‚é£y‚é¶ ‚é£1‚é¶

```

Solutions found using:

- Gaussian elimination
- Matrix inverse (if exists)

---

## 6. Vector Spaces

A **vector space** satisfies:

- Closure under addition
- Closure under scalar multiplication
- Zero vector exists
- Additive inverse exists

Examples:

- ‚Ñù‚Åø
- Polynomial spaces
- Function spaces

**AI intuition:**
Feature vectors live in vector spaces.

---

## 7. Linear Transformations

A function `T(v)` is linear if:

```

T(u + v) = T(u) + T(v)
T(cv) = cT(v)

```

Represented using matrices:

```

T(v) = Av

```

Examples:

- Rotation
- Scaling
- Reflection

---

## 8. Matrix Properties

### Transpose

```

A·µÄ

```

### Symmetric Matrix

```

A = A·µÄ

```

### Identity Matrix

```

I = ‚é°1 0‚é§
‚é£0 1‚é¶

```

### Zero Matrix

All elements are zero.

---

## 9. Determinants

For 2√ó2 matrix:

```

A = ‚é°a b‚é§
‚é£c d‚é¶

det(A) = ad ‚àí bc

```

**Meaning:**

- Area/volume scaling factor
- det = 0 ‚Üí matrix is singular (not invertible)

---

## 10. Matrix Inverse

If `A‚Åª¬π` exists:

```

A A‚Åª¬π = I

```

Solution to:

```

Ax = b

```

is:

```

x = A‚Åª¬πb

```

**AI note:**
Direct inversion is expensive; usually avoided in ML.

---

## 11. Rank and Null Space

### Rank

- Number of linearly independent columns

### Null Space

All vectors `x` such that:

```

Ax = 0

```

**Interpretation:**

- Rank ‚Üí information content
- Null space ‚Üí lost information

---

## 12. Eigenvalues and Eigenvectors

Definition:

```

Av = Œªv

```

- `v` ‚Üí eigenvector
- `Œª` ‚Üí eigenvalue

Example:

```

A = ‚é°2 0‚é§
‚é£0 3‚é¶

```

Eigenvalues: `2, 3`

**AI intuition:**

- Principal directions of data
- Core of PCA

---

## 13. Diagonalization

If possible:

```

A = PDP‚Åª¬π

```

Where:

- `D` is diagonal (eigenvalues)
- `P` contains eigenvectors

Speeds up:

- Matrix powers
- Computations

---

## 14. Orthogonality and Orthonormal Bases

### Orthogonal Vectors

```

v ¬∑ w = 0

```

### Orthonormal

- Orthogonal
- Unit length

Used in:

- QR decomposition
- PCA
- Signal processing

---

## 15. Projections

Projection of `v` onto `u`:

```

proj·µ§(v) = (v¬∑u / u¬∑u) u

```

**AI intuition:**

- Feature projection
- Dimensionality reduction

---

## 16. Singular Value Decomposition (SVD)

Any matrix:

```

A = U Œ£ V·µÄ

```

Where:

- `U` ‚Üí left singular vectors
- `Œ£` ‚Üí singular values
- `V` ‚Üí right singular vectors

Applications:

- PCA
- Noise reduction
- Recommendation systems

---

## 17. Norms and Distance Measures

### L2 Norm (Euclidean)

```

||v||‚ÇÇ = ‚àö(v‚ÇÅ¬≤ + v‚ÇÇ¬≤ + ...)

```

### L1 Norm

```

||v||‚ÇÅ = |v‚ÇÅ| + |v‚ÇÇ| + ...

```

### Cosine Similarity

```

(v¬∑w) / (||v|| ||w||)

```

Used in:

- Similarity search
- NLP embeddings

---

## 18. Quadratic Forms

```

x·µÄAx

```

Used in:

- Loss functions
- Optimization
- Regularization

Example:

- Mean Squared Error

---

## 19. Linear Algebra in Machine Learning

| Concept      | ML Application           |
| ------------ | ------------------------ |
| Vectors      | Feature representation   |
| Matrices     | Datasets, weights        |
| Dot product  | Predictions              |
| Eigenvectors | PCA                      |
| SVD          | Dimensionality reduction |
| Norms        | Regularization           |
| Projections  | Feature extraction       |

---

## 20. Summary Cheat Sheet

- **Data** ‚Üí vectors & matrices
- **Models** ‚Üí linear transformations
- **Training** ‚Üí solving linear systems
- **Optimization** ‚Üí quadratic forms
- **Dimensionality reduction** ‚Üí eigenvalues & SVD

> Mastering Linear Algebra = Understanding how AI thinks mathematically.

---

## Next Steps

- Study **Calculus for AI**
- Study **Probability & Statistics**
- Implement concepts using **NumPy & PyTorch**

---

### ‚≠ê If you found this useful, consider starring the repository!

```

```
