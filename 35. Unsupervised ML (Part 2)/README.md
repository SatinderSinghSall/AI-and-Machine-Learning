# Thyroid Disease Anomaly Detection using Unsupervised Machine Learning

## ğŸ“Œ Project Overview

This project implements and compares **four unsupervised machine learning algorithms** to detect anomalies (outliers) in a medical thyroid dataset.

Anomaly detection is critical in healthcare analytics because rare or abnormal records may indicate:

- Disease conditions
- Measurement errors
- Unusual patient profiles
- Data corruption

Since anomalies are **rare and often unlabeled**, we apply **unsupervised learning techniques** that detect abnormal behavior without relying on class labels.

---

## ğŸ¯ Objectives

The main goals of this project are:

1. Understand dimensionality reduction using PCA
2. Apply density-based anomaly detection (DBSCAN)
3. Apply tree-based anomaly detection (Isolation Forest)
4. Apply local density-based detection (LOF)
5. Compare performance and behavior of each algorithm
6. Visualize anomalies for interpretability

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ pca-algorithm.ipynb
â”œâ”€â”€ DBSCAN-Anomaly.ipynb
â”œâ”€â”€ Isolation-Forest-Algo.ipynb
â”œâ”€â”€ LOF-Algorithm.ipynb
â”œâ”€â”€ thyroid_dataset.csv
â””â”€â”€ README.md
```

---

# ğŸ“Š Dataset Description

## Thyroid Dataset

**File:** `thyroid_dataset.csv`

### Characteristics

- Samples: 6916 patients
- Features: 21 medical attributes
- Target: `Outlier_label`

### Example Features

- Age
- Sex
- Hormone measurements
- Medication indicators
- Diagnostic flags

### Why anomaly detection?

Medical datasets typically contain:

- Very few abnormal cases
- Imbalanced classes
- Hidden pathological patterns

Thus, unsupervised detection is appropriate.

---

# âš™ï¸ Algorithms Implemented

This project implements four major techniques:

| Algorithm        | Category                 | Purpose                   |
| ---------------- | ------------------------ | ------------------------- |
| PCA              | Dimensionality Reduction | Visualization             |
| DBSCAN           | Density-based            | Cluster + noise detection |
| Isolation Forest | Tree-based               | Global anomaly detection  |
| LOF              | Local density-based      | Local anomaly detection   |

---

---

# 1ï¸âƒ£ Principal Component Analysis (PCA)

## Notebook

`pca-algorithm.ipynb`

## Purpose

PCA is **not an anomaly detector**, but a **dimensionality reduction technique** used for:

- Visualization
- Noise reduction
- Feature compression
- Faster modeling

---

## Mathematical Intuition

PCA finds directions that maximize variance.

Steps:

1. Standardize features
2. Compute covariance matrix
3. Compute eigenvectors
4. Project data onto top components

Projection:

[
Z = XW
]

where

- X = original data
- W = principal components
- Z = reduced data

---

## Workflow

```
Load data
â†’ Standardize
â†’ Apply PCA (2 components)
â†’ Plot 2D projection
```

---

## Use Case in Project

Used to:

- Visualize anomalies detected by other algorithms
- Reduce high-dimensional thyroid data to 2D

---

---

# 2ï¸âƒ£ DBSCAN (Density-Based Spatial Clustering)

## Notebook

`DBSCAN-Anomaly.ipynb`

## Concept

DBSCAN groups points based on **density** rather than distance from centroids.

Key idea:

- Dense region â†’ cluster
- Sparse region â†’ anomaly

---

## Parameters

| Parameter   | Meaning                       |
| ----------- | ----------------------------- |
| eps         | neighborhood radius           |
| min_samples | minimum neighbors for cluster |

---

## Classification Rules

- Core point â†’ enough neighbors
- Border point â†’ near cluster
- Noise point â†’ anomaly

Noise label:

```
-1 â†’ anomaly
```

---

## Algorithm Steps

```
For each point:
   Count neighbors within eps
   If neighbors >= min_samples â†’ core
   Else â†’ noise
```

---

## Workflow

```
Generate data
â†’ Scale
â†’ Apply DBSCAN
â†’ Label clusters
â†’ Noise = anomaly
â†’ Visualize
```

---

## Advantages

âœ” No need to specify number of clusters
âœ” Detects arbitrary shapes
âœ” Automatically finds noise

## Limitations

âœ˜ Sensitive to eps
âœ˜ Struggles with varying density

---

---

# 3ï¸âƒ£ Isolation Forest

## Notebook

`Isolation-Forest-Algo.ipynb`

## Concept

Isolation Forest isolates anomalies using **random tree splits**.

Key principle:

> Anomalies are easier to isolate and require fewer splits.

---

## Mathematical Idea

Average path length:

[
score(x) = 2^{-\frac{E(h(x))}{c(n)}}
]

- shorter path â†’ more anomalous

---

## Parameters

| Parameter     | Meaning               |
| ------------- | --------------------- |
| n_estimators  | number of trees       |
| contamination | expected anomaly rate |
| random_state  | reproducibility       |

---

## Workflow

```
Load thyroid dataset
â†’ Scale features
â†’ Train Isolation Forest
â†’ Predict labels
â†’ PCA visualization
â†’ Count anomalies
```

---

## Advantages

âœ” Very fast
âœ” Works well for high dimensions
âœ” Scales to large datasets
âœ” Minimal parameter tuning

## Limitations

âœ˜ Global only (misses local anomalies sometimes)

---

---

# 4ï¸âƒ£ Local Outlier Factor (LOF)

## Notebook

`LOF-Algorithm.ipynb`

## Concept

LOF compares **local density** of a point to neighbors.

If a point is less dense than neighbors â†’ anomaly.

---

## Mathematical Intuition

Local reachability density:

[
LOF(p) = \frac{average\ density\ of\ neighbors}{density(p)}
]

If:

```
LOF â‰ˆ 1 â†’ normal
LOF >> 1 â†’ anomaly
```

---

## Workflow

```
Load thyroid dataset
â†’ Scale
â†’ Fit LOF
â†’ Predict outliers
â†’ PCA visualization
```

---

## Advantages

âœ” Detects local anomalies
âœ” Works well for clusters of varying density

## Limitations

âœ˜ Computationally expensive
âœ˜ Slower on large datasets

---

---

# ğŸ“ˆ Visual Pipeline (Complete Project)

```
Raw Dataset
    â†“
Standard Scaling
    â†“
Isolation Forest / LOF / DBSCAN
    â†“
Outlier Labels
    â†“
PCA (2D)
    â†“
Visualization
```

---

# ğŸ“Š Algorithm Comparison

| Method           | Type      | Speed     | Local detection | Best for            |
| ---------------- | --------- | --------- | --------------- | ------------------- |
| PCA              | Reduction | Fast      | No              | Visualization       |
| DBSCAN           | Density   | Medium    | Yes             | Non-linear shapes   |
| Isolation Forest | Tree      | Very Fast | No              | Large/high-dim data |
| LOF              | Density   | Slow      | Yes             | Local anomalies     |

---

# ğŸ§ª Suggested Evaluation Metrics

Since labels exist:

- Precision
- Recall
- F1-score
- ROC-AUC
- Confusion Matrix

Example:

```python
from sklearn.metrics import classification_report
print(classification_report(y_true, y_pred))
```

---

# â–¶ï¸ How to Run

## Install dependencies

```bash
pip install numpy pandas matplotlib seaborn scikit-learn jupyter
```

## Launch notebooks

```bash
jupyter notebook
```

Run notebooks in order:

1. pca-algorithm.ipynb
2. DBSCAN-Anomaly.ipynb
3. Isolation-Forest-Algo.ipynb
4. LOF-Algorithm.ipynb

---

# ğŸ’¼ Professional Applications

These methods are widely used in:

- Healthcare diagnostics
- Fraud detection
- Cybersecurity
- Fault detection
- Manufacturing QA
- Financial risk analysis

---

# ğŸ“š Key Takeaways

- PCA helps visualization
- DBSCAN detects shape-based noise
- Isolation Forest is fastest and scalable
- LOF detects subtle local anomalies
- Combining methods improves robustness

---

# ğŸš€ Future Improvements

Possible extensions:

- Hyperparameter tuning
- Cross-validation
- Ensemble anomaly detection
- Autoencoders (deep learning)
- SHAP explainability
- Real-time detection

---

# ğŸ‘¨â€ğŸ’» Author

Satinder Singh Sall

Machine Learning Anomaly Detection Project
Academic + Professional Demonstration

---

# ğŸ“œ License

For educational and research purposes only.

---

> **Unsupervised Anomaly Detection on Thyroid dataset + PCA visualization**

---

# ğŸ“ 1) `DBSCAN-Anomaly.ipynb`

**Goal:** Detect anomalies using **DBSCAN clustering** on synthetic data.

---

### Cell 0 â€” Markdown

Introduces:

- DBSCAN
- Density-based clustering
- Unsupervised anomaly detection

---

### Cell 1 â€” Imports

```python
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons
```

Used for:

- Visualization
- Scaling
- Clustering
- Synthetic dataset

---

### Cell 3 â€” Generate Data

```python
X, y = make_moons(n_samples=500, noise=0.1, random_state=42)
```

Creates:

- 500 curved moon-shaped points
- Noise added
- Perfect for DBSCAN because clusters are non-linear

---

### Cell 4

Displays raw data.

---

### Cell 6 â€” Scaling

```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

Why?

- DBSCAN uses **distance**
- Distance requires normalized features

---

### Cell 8 â€” Plot

```python
sns.scatterplot(...)
```

Shows shape of moons.

---

### Cell 10 â€” DBSCAN model

```python
DBSCAN(eps=0.18, min_samples=5)
```

Key parameters:

- `eps` â†’ neighborhood radius
- `min_samples` â†’ density threshold

---

### Cell 11 â€” Fit + predict

```python
labels = dbscan.fit_predict(X_scaled)
```

Outputs:

- cluster id
- `-1` = anomaly (noise)

---

### Cell 13 â€” Visualization

Colored scatter plot:

- clusters
- anomalies

---

### âœ… Summary

Pipeline:

```
Generate â†’ Scale â†’ DBSCAN â†’ Label â†’ Visualize
```

Good for:

- non-linear clusters
- density-based anomaly detection

---

# ğŸ“ 2) `Isolation-Forest-Algo.ipynb`

**Goal:** Detect anomalies in real **thyroid medical dataset**

---

### Cell 1 â€” Imports

Includes:

- pandas
- matplotlib
- StandardScaler
- IsolationForest
- PCA

---

### Cell 3 â€” Load dataset

```python
df = pd.read_csv("thyroid_dataset.csv")
```

Dataset:

```
Rows: 6916
Columns: 22
```

Includes:

- medical features
- `Outlier_label`

---

### Cell 6 â€” Separate X and y

```python
X = df.drop("Outlier_label", axis=1)
y = df["Outlier_label"]
```

Even though unsupervised, label is used for evaluation.

---

### Cell 8 â€” Scaling

```python
StandardScaler()
```

Important for tree isolation fairness.

---

### Cell 10 â€” Isolation Forest

```python
IsolationForest(
    n_estimators=200,
    contamination=0.036,
    random_state=42
)
```

Parameters:

- trees = 200
- contamination = expected anomaly %
- random_state = reproducible

---

### Cell 12 â€” Predict

```python
labels = clf.fit_predict(X_scaled)
```

Outputs:

- 1 = normal
- -1 = anomaly

---

### Cell 14â€“15 â€” PCA visualization

```python
PCA(n_components=2)
plt.scatter(...)
```

Used only to:

- reduce dimension
- plot anomalies

---

### Cell 17 â€” Count anomalies

```python
np.sum(labels == -1)
```

---

### âœ… Summary

Pipeline:

```
Load â†’ Scale â†’ IsolationForest â†’ Predict â†’ PCA â†’ Plot â†’ Count
```

Best for:

- high dimensional data
- fast
- robust

---

# ğŸ“ 3) `LOF-Algorithm.ipynb`

**Goal:** Compare **Isolation Forest + LOF**

---

### Cells 0â€“18

âš ï¸ **Duplicate of Isolation Forest section**
Runs same process as previous notebook first.

---

## New section starts at Cell 19

---

### Cell 20 â€” LOF model

```python
LocalOutlierFactor(contamination=0.036)
```

LOF logic:

- compares density of point vs neighbors
- lower density â†’ anomaly

---

### Cell 22 â€” Predict

```python
labels = neighbors.fit_predict(X_scaled)
```

Again:

- -1 anomaly
- 1 normal

---

### Cell 24â€“25 â€” PCA plot

Same as before.

---

### Cell 27 â€” Count anomalies

---

### âš ï¸ Issues to fix

1. Title still says Isolation Forest in LOF section
2. Unnecessary duplicate Isolation Forest block
3. Could compare results side-by-side

---

### âœ… Summary

Pipeline:

```
Load â†’ Scale â†’ LOF â†’ Predict â†’ PCA â†’ Plot
```

Best for:

- local density anomalies
- cluster-dependent outliers

---

# ğŸ“ 4) `pca-algorithm.ipynb`

**Goal:** Demonstrate PCA using Iris dataset

---

### Cell 1 â€” Imports

---

### Cell 3 â€” Load Iris

```python
load_iris()
```

Data:

- 150 samples
- 4 features
- 3 classes

---

### Cell 6 â€” Scaling

Important because PCA depends on variance.

---

### Cell 8 â€” PCA

```python
PCA(n_components=2)
```

Reduces:

```
4D â†’ 2D
```

---

### Cell 11 â€” Explained variance

```python
pca.explained_variance_ratio_
```

Shows:

- how much information kept

Usually ~95% for Iris

---

### Cell 13 â€” Plot

Colored by species.

---

### âœ… Summary

Pipeline:

```
Load â†’ Scale â†’ PCA â†’ Variance â†’ Plot
```

Pure dimensionality reduction demo.

---

# ğŸ“ 5) `thyroid_dataset.csv`

### Shape

```
6916 rows Ã— 22 columns
```

### Typical columns:

- Age
- Sex
- medication flags
- thyroid hormone levels
- Outlier_label

Used in:

- Isolation Forest
- LOF

---

# ğŸ¯ Big Picture (How all notebooks connect)

### Flow of your project:

```
PCA basics (iris)
      â†“
Synthetic anomaly detection (DBSCAN)
      â†“
Real dataset anomaly detection
      â†“
Compare algorithms:
   Isolation Forest
   LOF
```

---

# ğŸ“Š Algorithm Comparison

| Algorithm        | Strength             | Weakness          |
| ---------------- | -------------------- | ----------------- |
| DBSCAN           | shape-based clusters | sensitive eps     |
| Isolation Forest | fast, scalable       | global only       |
| LOF              | local density aware  | slow for big data |
| PCA              | visualization only   | not detector      |

---

# ğŸ’¡ Suggestions to Improve

### 1. Compare algorithms together

Add:

```python
from sklearn.metrics import classification_report
print(classification_report(y, labels))
```

---

### 2. Avoid duplicate code

Create reusable functions:

```python
def scale_data(X):
    return StandardScaler().fit_transform(X)
```

---

### 3. Tune contamination automatically

```python
contamination = y.mean()
```

---

### 4. Add metrics

- Precision
- Recall
- F1
- ROC curve
