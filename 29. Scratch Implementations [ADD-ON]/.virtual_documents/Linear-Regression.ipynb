


import numpy as np





class LinearRegression:
    def __init__(self, learning_rate=0.01, n_iter=1000):
        self.bias = None
        self.weights = None
        self.lr = learning_rate
        self.n_iter = n_iter

    def fit(self, X, y):
        m, n = X.shape  # samples, features

        # Step 1: Initialise parameters
        self.bias = 0
        self.weights = np.zeros(n)

        # Gradient Descent
        for _ in range(self.n_iter):
            # Step 2: Predictions
            y_pred = self.bias + np.dot(X, self.weights)

            # Step 3: Gradients
            db = (1 / m) * np.sum(y_pred - y)
            dw = (1 / m) * np.dot(X.T, (y_pred - y))

            # Step 4: Update parameters
            self.bias -= self.lr * db
            self.weights -= self.lr * dw

    def predict(self, X):
        return self.bias + np.dot(X, self.weights)


# Data
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

# Train model
model = LinearRegression()
model.fit(X, y)

# Predict
y_pred = model.predict(X)
print(y_pred)

# Print Bias & Weights:
print(model.bias)
print(model.weights)









